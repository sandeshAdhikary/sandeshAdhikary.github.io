<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Home</title>
  <meta name="description" content="My personal website">

  <!-- Google Fonts loaded here depending on setting in _data/options.yml true loads font, blank does not-->
  
    <link href='//fonts.googleapis.com/css?family=Lato:400,400italic' rel='stylesheet' type='text/css'>
  
  
  <!-- Load up MathJax script if needed ... specify in /_data/options.yml file-->
  
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  

  <link rel="stylesheet" type="text/css" href="/sandeshAdhikary.github.io/css/tufte.css">
  <link rel="stylesheet" type="text/css" href="/sandeshAdhikary.github.io/css/latex.css">
  <!-- <link rel="stylesheet" type="text/css" href="/sandeshAdhikary.github.io/css/print.css" media="print"> -->

  <link rel="canonical" href="/sandeshAdhikary.github.io/">

  <link rel="alternate" type="application/rss+xml" title="sandesh" href="/sandeshAdhikary.github.io/feed.xml" />
</head>

  <body>
    <!--- Header and nav template site-wide -->
<header>
	

    <nav class="group">
	
	
		
		    
		      <a class="active" href="/sandeshAdhikary.github.io/" class="active">Home</a>
		    
	    
  	
		
  	
		
  	
		
		    
<!--		uncomment to bring back other nav links-->
<!--		      <a href="/sandeshAdhikary.github.io/research-journal">Research Journal</a>-->
		    
	    
  	
		
  	
		
		    
<!--		uncomment to bring back other nav links-->
<!--		      <a href="/sandeshAdhikary.github.io/blog/">Blog</a>-->
		    
	    
  	
		
		    
<!--		uncomment to bring back other nav links-->
<!--		      <a href="/sandeshAdhikary.github.io/page/">Tufte CSS</a>-->
		    
	    
  	
	</nav>
</header>
    <article class="group">
      <script defer src="assets/js/modals.js"></script>
<link href="css/modal_styles.css" rel="stylesheet">

<!-- Homepage -->

<!--Title-->
    <h1>Sandesh Adhikary</h1>
    <p class="subtitle">
        PhD Student, UW CSE
    </p>

<section>
<!--    Intro-->
    <p>
        I'm a PhD student
        at the <a href="https://www.cs.washington.edu/">Paul G. Allen School of Computer Science at the University of Washington</a>,
        advised by <a href="https://homes.cs.washington.edu/~bboots/">Professor Byron Boots</a>.

        <span class="marginnote">
            <img class = "fullwidth" src="assets/img/me.jpg">
        </span>
    </p>

    <p>
        My research has been focused on quantum-inspired probabilistic models for sequential data.
        In particular, Iâ€™ve worked on understanding how these models relate to models from classical machine learning in terms of expressiveness and learnability.
        Currently, I am working on applying some of these models from stochastic processes and tensor networks to the problem of learning parameters of quantum systems from experimental data.
        Recently, I have also been interested in kernel-based sampling methods, particularly when applied to problems that require sampling over non-Euclidean Riemannian manifolds.
    </p>
</section>

<section>
<!--    Research-->
    <h1>Projects</h1>

<!--    Project -->
    <section>
        <h3>Expressiveness and Learning of <br/> Hidden Quantum Markov Models</h3>
        <br/>
        <!--    Abstract Modal-->
        <a data-modal-target="#aistats2020_abstract" class="home">[Abstract]</a>
        <div class="modal" id="aistats2020_abstract">
            <div class="modal-header">
                <div class="title">Expressiveness and Learning of Hidden Quantum Markov Models</div>
                <button data-close-button class="close-button">&times;</button>
            </div>
            <div class="modal-body">
            Extending classical probabilistic reasoning using the quantum mechanical view of probability has been of recent interest, particularly in the development of hidden quantum Markov models (HQMMs) to model stochastic processes.
            However, there has been little progress in characterizing the expressiveness of such models and learning them from data.
            We tackle these problems by showing that HQMMs are a special subclass of the general class of observable operator models (OOMs) that do not suffer from the negative probability problem by design. We also provide a feasible retraction-based learning algorithm for HQMMs using constrained gradient descent on the Stiefel manifold of model parameters.
            We demonstrate that this approach is faster and scales to larger models than previous learning algorithms.
            We also provide a feasible retraction-based learning algorithm for HQMMs.
            </div>
        </div>
        <div id="overlay"></div>
        <a href="http://proceedings.mlr.press/v108/adhikary20a/adhikary20a.pdf" class="home">[PDF]</a>
        <a href="https://arxiv.org/pdf/1912.02098.pdf" class="home">[arXiv]</a>
        <!--    Bibtex Modal-->
        <a data-modal-target="#aistats2020_bibtex" class="home">[BibTex]</a>
        <div class="modal" id="aistats2020_bibtex">
            <div class="modal-header">
                <div class="title">Expressiveness and Learning of Hidden Quantum Markov Models</div>
                <button data-close-button class="close-button">&times;</button>
            </div>

            <div class="modal-body">
                @InProceedings{pmlr-v108-adhikary20a, title = {Expressiveness and Learning of Hidden Quantum Markov Models}, author = {Adhikary, Sandesh and Srinivasan, Siddarth and Gordon, Geoff and Boots, Byron}, booktitle = {Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics}, pages = {4151--4161}, year = {2020}, editor = {Silvia Chiappa and Roberto Calandra}, volume = {108}, series = {Proceedings of Machine Learning Research}, month = {26--28 Aug}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v108/adhikary20a/adhikary20a.pdf}, url = { http://proceedings.mlr.press/v108/adhikary20a.html }}
            </div>
        </div>
        <div id="overlay"></div>

        <p>

            <label for="aistats2020_cite" class="margin-toggle sidenote-number">Our paper</label> at AISTATS (2020):
            <input type="checkbox" id="aistats2020_cite" class="margin-toggle"/>
            <span class="sidenote">
                <a href="http://proceedings.mlr.press/v108/adhikary20a">
                <i>Expressiveness and Learning of Hidden Quantum Markov Models</i>,
                <b>S. Adhikary</b>, S. Srinivasan, G. Gordon & B. Boots,
                International Conference on Artificial Intelligence and Statistics (2020)
                </a>
            </span>
            Extending classical probabilistic reasoning using the quantum mechanical view
            of probability has been of recent interest, particularly in the development of hidden quantum Markov models (HQMMs) to model stochastic processes.
            However, there has been little progress in characterizing the expressiveness of such models and learning them from data. We tackle these problems by showing that HQMMs are a special subclass of the general class of observable operator models (OOMs) that do not suffer from the negative probability problem by design.
        </p>
        <figure>
            <img class = "fullwidth" src="assets/img/aistats2020_thumbnail.png" align="center" width = "100%" height="100%">
            <figcaption>
                Conversions between various representations of quantum channels that constitute HQMMs.
            </figcaption>
        </figure>

    </section>

    <!--    Project -->
    <section>
        <h3>Quantum Tensor Networks, <br/> Stochastic Processes, and Weighted Automata</h3>
        <br/>
        <!--    Abstract Modal-->
        <a data-modal-target="#aistats2021_abstract" class="home">[Abstract]</a>
        <div class="modal" id="aistats2021_abstract">
            <div class="modal-header">
                <div class="title">Quantum Tensor Netoworks, Stochastic Processes, and Weighted Automata</div>
                <button data-close-button class="close-button">&times;</button>
            </div>
            <div class="modal-body">
            Modeling joint probability distributions over sequences has been studied from many perspectives. The physics community developed matrix product states, a tensor-train decomposition for probabilistic modeling, motivated by the need to tractably model many-body systems. But similar models have also been studied in the stochastic processes and weighted automata literature, with little work on how these bodies of work relate to each other. We address this gap by showing how stationary or uniform versions of popular quantum tensor network models have equivalent representations in the stochastic processes and weighted automata literature, in the limit of infinitely long sequences. We demonstrate several equivalence results between models used in these three communities: (i) uniform variants of matrix product states, Born machines and locally purified states from the quantum tensor networks literature, (ii) predictive state representations, hidden Markov models, norm-observable operator models and hidden quantum Markov models from the stochastic process literature, and (iii) stochastic weighted automata, probabilistic automata and quadratic automata from the formal languages literature. Such connections may open the door for results and methods developed in one area to be applied in another.
            </div>
        </div>
        <div id="overlay"></div>
        <a href="http://proceedings.mlr.press/v108/adhikary20a/adhikary20a.pdf" class="home">[PDF]</a>
        <a href="https://arxiv.org/pdf/1912.02098.pdf" class="home">[arXiv]</a>
        <!--    Bibtex Modal-->
        <a data-modal-target="#aistats2021_bibtex" class="home">[BibTex]</a>
        <div class="modal" id="aistats2021_bibtex">
            <div class="modal-header">
                <div class="title">Quantum Tensor Netoworks, Stochastic Processes, and Weighted Automata</div>
                <button data-close-button class="close-button">&times;</button>
            </div>

            <div class="modal-body">
                @InProceedings{pmlr-v130-adhikary21a, title = { Quantum Tensor Networks, Stochastic Processes, and Weighted Automata }, author = {Adhikary, Sandesh and Srinivasan, Siddarth and Miller, Jacob and Rabusseau, Guillaume and Boots, Byron}, booktitle = {Proceedings of The 24th International Conference on Artificial Intelligence and Statistics}, pages = {2080--2088}, year = {2021}, editor = {Arindam Banerjee and Kenji Fukumizu}, volume = {130}, series = {Proceedings of Machine Learning Research}, month = {13--15 Apr}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v130/adhikary21a/adhikary21a.pdf}, url = { http://proceedings.mlr.press/v130/adhikary21a.html }}  </div>
        </div>
        <div id="overlay"></div>


        <p>

            <label for="aistats2021_cite" class="margin-toggle sidenote-number"> Our paper</label> at AISTATS (2021),
            <input type="checkbox" id="aistats2021_cite" class="margin-toggle"/>
            <span class="sidenote">
                <a href="http://proceedings.mlr.press/v130/adhikary21a.html">
                <i>Quantum Tensor Networks, Stochastic Processes, & Weighted Automata</i>,
                <b>S. Adhikary</b>, S. Srinivasan, J. Miller, G. Rabusseau, B. Boots,
                24th International Conference on Artificial Intelligence and Statistics 2021
                </a>
            </span>

            <label for="neurips2021_cite" class="margin-toggle sidenote-number"> which began as a workshop paper</label>
            at the <a href="https://tensorworkshop.github.io/NeurIPS2020/">quantum tensor networks workshop at Neurips (2020)</a>:
            <input type="checkbox" id="neurips2021_cite" class="margin-toggle"/>
            <span class="sidenote">
                <a href="https://tensorworkshop.github.io/NeurIPS2020/accepted_papers/QTN__arXiv___final_NeurIPS_Copy_.pdf">
                <i>Quantum Tensor Networks, Stochastic Processes, & Weighted Automata</i>,
                S. Srinivasan, <b>S. Adhikary</b> , J. Miller, G. Rabusseau, B. Boots,
                Workshop on Quantum Tensor Networks in Machine Learning, 34th Conference on Neural Information Processing Systems 2021
                </a>
            </span>
            Modeling joint probability distributions over sequences has been studied from many perspectives.
            The physics community developed matrix product states, motivated by the need to tractably model many-body systems.
            But similar models have also been
            studied in the stochastic processes and weighted automata literature, with little work on how these bodies
            of work relate to each other. We address this gap by showing how stationary or uniform versions of popular
            quantum tensor network models have equivalent representations in the stochastic processes and weighted
            automata literature, in the limit of infinitely long sequences.
        </p>
        <figure>
            <img class = "fullwidth" src="assets/img/aistats2021_thumbnail_sets.png" align="center" width = "100%" height="100%">
            <figcaption>
                Subset relationships between the various models we considered. The shaded region is potentially empty.
            </figcaption>
        </figure>

    </section>

</section>



    </article>
    <span class="print-footer">Home - Sandesh Adhikary</span>
    <footer>
  <hr class="slender">
<div class="credits">
<span>&copy; 2021 
  
		<a href="mailto:adhikary [at] cs [dot] washington [dot] edu">Sandesh Adhikary</a></span></br> <br>    
    

<span>Created with <a href="//jekyllrb.com">Jekyll</a> using the <a href="//github.com/sdruskat/tufte-css-jekyll">tufte-css-jekyll</a>
    theme from <a href="https://github.com/sdruskat/tufte-css-jekyll"> Stephan Druskat</a></span>
</div>  
</footer>
  </body>
</html>
